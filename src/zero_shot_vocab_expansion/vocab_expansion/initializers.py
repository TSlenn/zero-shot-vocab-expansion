import numpy as np
import warnings
from ..embedding_model import EmbeddingModel
from ..dataset import VocabDataset


def get_distribution_params(weights: np.ndarray):
    """Calculates mean and covariance matrix of weights array.

    Args:
        weights (np.ndarray): (n, k) weights array of vocab embeddings.

    Returns:
        tuple containing:
        - np.ndarray: (k,) column means.
        - np.ndarray: (k, k) covariance matrix.

    """
    mu = np.mean(weights, axis=0)
    n = weights.shape[0]
    sigma = ((weights - mu).T @ (weights - mu)) / n
    return mu, sigma


def random_initializer(embeddings: np.ndarray, n: int):
    """Randomly generates new embedding vectors about distribution center.

    Args:
        embeddings (np.ndarray): (n_old, k) weights array of pretrained
            embeddings.
        n (int): Number of new embedding vectors to initialize.

    Returns:
        np.ndarray: (n, k) array of new embeddings.

    """
    mu, sigma = get_distribution_params(embeddings)
    # Ignore false warnings generated by numpy
    warnings.filterwarnings("ignore")
    new_embeddings = np.random.multivariate_normal(
        mu, cov=1e-5*sigma, size=n
    )
    return new_embeddings


def model_initializer(model: EmbeddingModel, words: list, definitions: dict):
    """Finds definitions and applies pretrained EmbeddingModel.

    Args:
        model (EmbeddingModel): Model trained to process definitions into
            embedding vectors.
        words (list of str): New vocabulary to process through model.
        definitions (dict): User provided word definitions.

    Returns:
        dict: {str: Tensor} of output embeddings.

    """
    ds = VocabDataset.from_list(words, definitions)
    ds_dict = {x.guid: x.texts[0] for x in ds}
    embeddings = model.encode(list(ds_dict.values()))
    return {word: emb for word, emb in zip(ds_dict.keys(), embeddings)}
